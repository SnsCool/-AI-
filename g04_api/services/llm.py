"""
LLMã‚µãƒ¼ãƒ“ã‚¹ - Google Geminié€£æº
"""

import os
from typing import Tuple

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


class LLMService:
    """Google Gemini ã‚µãƒ¼ãƒ“ã‚¹"""

    SYSTEM_PROMPT_KNOWLEDGE = """ã‚ãªãŸã¯ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®ãƒ«ãƒ¼ãƒ«:
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦æ­£ç¢ºã«å›ç­”ã™ã‚‹
2. å‡ºå…¸ã‚’[å‡ºå…¸N]ã®å½¢å¼ã§æ˜è¨˜ã™ã‚‹
3. ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã™ã‚‹

å›ç­”å½¢å¼:
- ç®‡æ¡æ›¸ãã‚’æ´»ç”¨ã™ã‚‹
- æ‰‹é †ãŒã‚ã‚‹å ´åˆã¯ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’ä½¿ç”¨
- é‡è¦ãªæƒ…å ±ã¯å¼·èª¿ã™ã‚‹"""

    SYSTEM_PROMPT_GENERAL = """ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§å½¹ç«‹ã¤å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®ãƒ«ãƒ¼ãƒ«:
1. ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã™ã‚‹
2. ä¸ç¢ºã‹ãªæƒ…å ±ã¯ã€Œç¢ºå®Ÿã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã¨å‰ç½®ãã™ã‚‹
3. å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®è³ªå•ã‚’ä¿ƒã™

å›ç­”å½¢å¼:
- ç®‡æ¡æ›¸ãã‚’æ´»ç”¨ã™ã‚‹
- æ‰‹é †ãŒã‚ã‚‹å ´åˆã¯ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’ä½¿ç”¨"""

    def __init__(self):
        self.model = None
        self.general_model = None
        if GEMINI_AVAILABLE:
            api_key = os.getenv("GEMINI_API_KEY")
            if api_key:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel(
                    model_name="gemini-2.0-flash",
                    system_instruction=self.SYSTEM_PROMPT_KNOWLEDGE
                )
                self.general_model = genai.GenerativeModel(
                    model_name="gemini-2.0-flash",
                    system_instruction=self.SYSTEM_PROMPT_GENERAL
                )

    async def generate_answer(
        self,
        query: str,
        context: str
    ) -> Tuple[str, float]:
        """
        ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã§è³ªå•ã«å›ç­”

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            context: æ¤œç´¢ã§å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            Tuple[str, float]: (å›ç­”, ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢)
        """
        if self.model:
            return await self._generate_with_gemini(query, context)

        return self._generate_mock(query, context)

    async def generate_general_answer(
        self,
        query: str
    ) -> Tuple[str, float]:
        """
        ä¸€èˆ¬çŸ¥è­˜ã§è³ªå•ã«å›ç­”ï¼ˆãƒŠãƒ¬ãƒƒã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•

        Returns:
            Tuple[str, float]: (å›ç­”, ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢)
        """
        if self.general_model:
            try:
                prompt = f"""
è³ªå•: {query}

ä¸Šè¨˜ã®è³ªå•ã«å¯¾ã—ã¦ã€ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ã«ã¯è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã‹ã£ãŸãŸã‚ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§å›ç­”ã—ã¦ã„ã¾ã™ã€‚
"""
                response = self.general_model.generate_content(prompt)
                answer = "ğŸ“Œ **ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ã«ã¯è©²å½“æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªå›ç­”ã§ã™ï¼š**\n\n" + response.text
                return answer, 0.6

            except Exception as e:
                print(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")

        return (
            "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã”è³ªå•ã«å›ç­”ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "åˆ¥ã®è³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚",
            0.3
        )

    async def _generate_with_gemini(
        self,
        query: str,
        context: str
    ) -> Tuple[str, float]:
        """Google Geminiã§ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®å›ç­”ã‚’ç”Ÿæˆ"""
        try:
            prompt = f"""
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}

è³ªå•: {query}

ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
            response = self.model.generate_content(prompt)
            answer = response.text

            confidence = self._estimate_confidence(answer, context)

            return answer, confidence

        except Exception as e:
            print(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_mock(query, context)

    def _generate_mock(
        self,
        query: str,
        context: str
    ) -> Tuple[str, float]:
        """ãƒ¢ãƒƒã‚¯å›ç­”ã‚’ç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        if "[å‡ºå…¸1]" in context:
            lines = context.split("\n")
            content_start = False
            answer_parts = []

            for line in lines[:10]:
                if "[å‡ºå…¸1]" in line:
                    content_start = True
                    continue
                if content_start and line.strip():
                    if "[å‡ºå…¸2]" in line:
                        break
                    answer_parts.append(line)

            if answer_parts:
                answer = "\n".join(answer_parts[:5])
                answer += "\n\nè©³ç´°ã¯å‡ºå…¸ã‚’ã”ç¢ºèªãã ã•ã„ã€‚[å‡ºå…¸1][å‡ºå…¸2]"
                return answer, 0.85

        return (
            "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã”è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã™ã‚‹ã‹ã€æ‹…å½“éƒ¨ç½²ã«ç›´æ¥ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
            0.3
        )

    def _estimate_confidence(self, answer: str, context: str) -> float:
        """å›ç­”ã®ä¿¡é ¼åº¦ã‚’æ¨å®š"""
        confidence = 0.7

        if "[å‡ºå…¸" in answer:
            confidence += 0.15

        uncertain_phrases = [
            "ã‚ã‹ã‚Šã¾ã›ã‚“", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ä¸æ˜", "æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“",
            "ç¢ºèªã§ãã¾ã›ã‚“", "è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        ]
        for phrase in uncertain_phrases:
            if phrase in answer:
                confidence -= 0.2
                break

        if len(answer) < 50:
            confidence -= 0.1

        return max(0.0, min(1.0, round(confidence, 2)))
