#!/usr/bin/env python3
"""
Notionç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- notion_docså†…ã®markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰S3ç”»åƒURLã‚’æŠ½å‡º
- ç”»åƒã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- markdownã®ãƒªãƒ³ã‚¯ã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«æ›¸ãæ›ãˆ
"""

import os
import re
import requests
import hashlib
import time
from pathlib import Path
from urllib.parse import urlparse, unquote
from collections import defaultdict

# è¨­å®š
NOTION_DOCS_DIR = "/Users/hatakiyoto/-AI-egent-libvela/notion_docs"
S3_PATTERN = r'https://prod-files-secure\.s3\.us-west-2\.amazonaws\.com/[^)\s\]"]+'
REQUEST_DELAY = 0.2  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

# çµ±è¨ˆ
stats = {
    "total_images": 0,
    "downloaded": 0,
    "failed": 0,
    "skipped": 0,
    "total_bytes": 0,
    "by_folder": defaultdict(lambda: {"count": 0, "bytes": 0})
}

def get_file_extension(url, content_type=None):
    """URLã¾ãŸã¯Content-Typeã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—"""
    # URLã®ãƒ‘ã‚¹ã‹ã‚‰æ‹¡å¼µå­ã‚’æŠ½å‡º
    parsed = urlparse(url)
    path = unquote(parsed.path)

    # ãƒ‘ã‚¹ã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—
    ext_match = re.search(r'\.([a-zA-Z0-9]+)(?:\?|$)', path)
    if ext_match:
        ext = ext_match.group(1).lower()
        if ext in ['jpg', 'jpeg', 'png', 'gif', 'webp', 'svg', 'bmp', 'ico']:
            return f".{ext}"

    # Content-Typeã‹ã‚‰åˆ¤æ–­
    if content_type:
        type_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp',
            'image/svg+xml': '.svg',
            'image/bmp': '.bmp',
            'image/x-icon': '.ico',
        }
        for mime, extension in type_map.items():
            if mime in content_type:
                return extension

    return '.jpg'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def download_image(url, save_path):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        # æ‹¡å¼µå­ã‚’ç¢ºèªãƒ»ä¿®æ­£
        content_type = response.headers.get('Content-Type', '')
        expected_ext = get_file_extension(url, content_type)

        # ä¿å­˜ãƒ‘ã‚¹ã®æ‹¡å¼µå­ã‚’ä¿®æ­£
        current_ext = save_path.suffix.lower()
        if current_ext != expected_ext:
            save_path = save_path.with_suffix(expected_ext)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜
        with open(save_path, 'wb') as f:
            f.write(response.content)

        return save_path, len(response.content)

    except Exception as e:
        print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return None, 0

def process_markdown_file(md_path):
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ç”»åƒURLã‚’å‡¦ç†"""
    with open(md_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # S3 URLã‚’æ¤œç´¢
    urls = re.findall(S3_PATTERN, content)

    if not urls:
        return 0

    folder_path = md_path.parent
    assets_dir = folder_path / "assets"
    folder_name = folder_path.name

    modified = False
    image_count = 0

    for i, url in enumerate(urls):
        stats["total_images"] += 1

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆURLã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼‰
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        ext = get_file_extension(url)
        filename = f"image_{i+1:03d}_{url_hash}{ext}"
        save_path = assets_dir / filename

        print(f"  ğŸ“¥ ç”»åƒ {i+1}/{len(urls)}: {filename}")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        actual_path, size = download_image(url, save_path)

        if actual_path:
            # markdownã®URLã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«ç½®æ›
            relative_path = f"./assets/{actual_path.name}"
            content = content.replace(url, relative_path)
            modified = True
            image_count += 1

            stats["downloaded"] += 1
            stats["total_bytes"] += size
            stats["by_folder"][folder_name]["count"] += 1
            stats["by_folder"][folder_name]["bytes"] += size

            print(f"    âœ… ä¿å­˜å®Œäº† ({size/1024:.1f} KB)")
        else:
            stats["failed"] += 1

        time.sleep(REQUEST_DELAY)

    # å¤‰æ›´ãŒã‚ã‚Œã°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    if modified:
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(content)

    return image_count

def format_size(bytes_size):
    """ãƒã‚¤ãƒˆæ•°ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    if bytes_size < 1024:
        return f"{bytes_size} B"
    elif bytes_size < 1024 * 1024:
        return f"{bytes_size/1024:.1f} KB"
    elif bytes_size < 1024 * 1024 * 1024:
        return f"{bytes_size/(1024*1024):.1f} MB"
    else:
        return f"{bytes_size/(1024*1024*1024):.2f} GB"

def main():
    print("=" * 60)
    print("Notionç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)

    # å…¨markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    md_files = list(Path(NOTION_DOCS_DIR).rglob("index.md"))
    print(f"\nğŸ“ å‡¦ç†å¯¾è±¡: {len(md_files)} ãƒ•ã‚¡ã‚¤ãƒ«\n")

    for i, md_path in enumerate(md_files):
        relative_path = md_path.relative_to(NOTION_DOCS_DIR)

        # S3 URLãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        if 'prod-files-secure.s3.us-west-2.amazonaws.com' in content:
            print(f"\n[{i+1}/{len(md_files)}] ğŸ“„ {relative_path.parent}")
            count = process_markdown_file(md_path)
            if count > 0:
                print(f"  â†’ {count} ç”»åƒã‚’å‡¦ç†")

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    print(f"\nåˆè¨ˆç”»åƒæ•°: {stats['total_images']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}")
    print(f"\nğŸ’¾ åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}")

    # ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚ºï¼ˆå¤§ãã„é †ï¼‰
    if stats["by_folder"]:
        print("\nğŸ“ ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚ºï¼ˆä¸Šä½20ä»¶ï¼‰:")
        sorted_folders = sorted(
            stats["by_folder"].items(),
            key=lambda x: x[1]["bytes"],
            reverse=True
        )[:20]

        for folder, data in sorted_folders:
            print(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}")

    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_path = Path(NOTION_DOCS_DIR).parent / "image_download_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Notionç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"åˆè¨ˆç”»åƒæ•°: {stats['total_images']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}\n")
        f.write(f"åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}\n\n")
        f.write("ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚º:\n")
        for folder, data in sorted(stats["by_folder"].items(), key=lambda x: x[1]["bytes"], reverse=True):
            f.write(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}\n")

    print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    print("\nå®Œäº†!")

if __name__ == "__main__":
    main()
