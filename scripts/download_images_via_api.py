#!/usr/bin/env python3
"""
Notion APIçµŒç”±ã§ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- APIã‹ã‚‰æ–°ã—ã„ç½²åä»˜ãURLã‚’å–å¾—
- notion_images/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
- markdownã®ãƒªãƒ³ã‚¯ã‚’æ›´æ–°
"""

import os
import re
import json
import requests
import hashlib
import time
from pathlib import Path
from urllib.parse import urlparse, unquote
from collections import defaultdict
from dotenv import load_dotenv

# è¨­å®š
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_API_TOKEN")
NOTION_DOCS_DIR = Path("/Users/hatakiyoto/-AI-egent-libvela/notion_docs")
NOTION_IMAGES_DIR = Path("/Users/hatakiyoto/-AI-egent-libvela/notion_images")
STRUCTURE_FILE = Path("/Users/hatakiyoto/-AI-egent-libvela/notion_data/pages/verified_structure.json")

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json"
}

REQUEST_DELAY = 0.35  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

# çµ±è¨ˆ
stats = {
    "total_pages": 0,
    "pages_with_images": 0,
    "total_images": 0,
    "downloaded": 0,
    "failed": 0,
    "total_bytes": 0,
    "by_folder": defaultdict(lambda: {"count": 0, "bytes": 0})
}

def get_file_extension(url, content_type=None):
    """URLã¾ãŸã¯Content-Typeã‹ã‚‰æ‹¡å¼µå­ã‚’å–å¾—"""
    parsed = urlparse(url)
    path = unquote(parsed.path)

    ext_match = re.search(r'\.([a-zA-Z0-9]+)(?:\?|$)', path)
    if ext_match:
        ext = ext_match.group(1).lower()
        if ext in ['jpg', 'jpeg', 'png', 'gif', 'webp', 'svg', 'bmp', 'ico']:
            return f".{ext}"

    if content_type:
        type_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp',
            'image/svg+xml': '.svg',
        }
        for mime, extension in type_map.items():
            if mime in content_type:
                return extension

    return '.jpg'

def download_image(url, save_path):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()

        content_type = response.headers.get('Content-Type', '')
        expected_ext = get_file_extension(url, content_type)

        if save_path.suffix.lower() != expected_ext:
            save_path = save_path.with_suffix(expected_ext)

        save_path.parent.mkdir(parents=True, exist_ok=True)

        with open(save_path, 'wb') as f:
            f.write(response.content)

        return save_path, len(response.content)

    except Exception as e:
        print(f"    âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {str(e)[:100]}")
        return None, 0

def get_page_blocks(page_id):
    """ãƒšãƒ¼ã‚¸ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—ï¼ˆç”»åƒã‚’å«ã‚€ï¼‰"""
    blocks = []
    url = f"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100"

    while url:
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            data = response.json()

            for block in data.get("results", []):
                blocks.append(block)
                # å­ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯å†å¸°çš„ã«å–å¾—
                if block.get("has_children") and block["type"] not in ["child_page", "child_database"]:
                    child_blocks = get_page_blocks(block["id"])
                    blocks.extend(child_blocks)

            if data.get("has_more"):
                url = f"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100&start_cursor={data['next_cursor']}"
            else:
                url = None

            time.sleep(REQUEST_DELAY)

        except Exception as e:
            print(f"    âš ï¸ ãƒ–ãƒ­ãƒƒã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
            break

    return blocks

def extract_image_urls(blocks):
    """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º"""
    images = []

    for block in blocks:
        block_type = block.get("type")

        if block_type == "image":
            image_data = block.get("image", {})
            image_type = image_data.get("type")

            if image_type == "file":
                url = image_data.get("file", {}).get("url")
                if url:
                    images.append({
                        "url": url,
                        "type": "notion_file",
                        "block_id": block["id"]
                    })
            elif image_type == "external":
                url = image_data.get("external", {}).get("url")
                if url:
                    images.append({
                        "url": url,
                        "type": "external",
                        "block_id": block["id"]
                    })

        # calloutãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¢ã‚¤ã‚³ãƒ³ç”»åƒ
        elif block_type == "callout":
            icon = block.get("callout", {}).get("icon", {})
            if icon and icon.get("type") == "file":
                url = icon.get("file", {}).get("url")
                if url:
                    images.append({
                        "url": url,
                        "type": "notion_file",
                        "block_id": block["id"]
                    })

    return images

def sanitize_folder_name(name):
    """ãƒ•ã‚©ãƒ«ãƒ€åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
    # ç„¡åŠ¹ãªæ–‡å­—ã‚’ç½®æ›
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '_')
    return name.strip()

def get_relative_path(image_path, md_path):
    """markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—"""
    try:
        return os.path.relpath(image_path, md_path.parent)
    except ValueError:
        return str(image_path)

def update_markdown_links(md_path, old_urls, new_paths):
    """markdownãƒ•ã‚¡ã‚¤ãƒ«å†…ã®URLã‚’ç›¸å¯¾ãƒ‘ã‚¹ã«æ›´æ–°"""
    if not old_urls or not new_paths:
        return

    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        modified = False
        for old_url, new_path in zip(old_urls, new_paths):
            if new_path and old_url in content:
                # S3 URLã®åŸºæœ¬éƒ¨åˆ†ã§ãƒãƒƒãƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿éƒ¨åˆ†ã¯å¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
                # URLã®åŸºæœ¬éƒ¨åˆ†ã‚’æŠ½å‡º
                base_url_match = re.search(r'(https://prod-files-secure\.s3\.us-west-2\.amazonaws\.com/[^?]+)', old_url)
                if base_url_match:
                    base_url = base_url_match.group(1)
                    # åŒã˜åŸºæœ¬URLã‚’æŒã¤ã™ã¹ã¦ã®URLã‚’ç½®æ›
                    pattern = re.escape(base_url) + r'[^)\s\]"]*'
                    relative_path = get_relative_path(new_path, md_path)
                    content = re.sub(pattern, relative_path, content)
                    modified = True

        if modified:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return True
    except Exception as e:
        print(f"    âš ï¸ Markdownæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    return False

def process_page(page_info, folder_path_map):
    """1ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†"""
    page_id = page_info["id"]
    page_title = page_info.get("title", "untitled")
    page_path = page_info.get("path", [])

    # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    if page_path:
        folder_name = "/".join([sanitize_folder_name(p) for p in page_path])
    else:
        folder_name = sanitize_folder_name(page_title)

    # ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
    blocks = get_page_blocks(page_id)
    images = extract_image_urls(blocks)

    if not images:
        return 0

    stats["pages_with_images"] += 1
    print(f"\nğŸ“„ {page_title} ({len(images)}ç”»åƒ)")

    # ç”»åƒä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€
    image_folder = NOTION_IMAGES_DIR / folder_name

    # å¯¾å¿œã™ã‚‹markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    md_path = None
    for candidate_path in [
        NOTION_DOCS_DIR / folder_name / "index.md",
        NOTION_DOCS_DIR / sanitize_folder_name(page_title) / "index.md"
    ]:
        if candidate_path.exists():
            md_path = candidate_path
            break

    downloaded_count = 0
    old_urls = []
    new_paths = []

    for i, img in enumerate(images):
        stats["total_images"] += 1
        url = img["url"]

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        ext = get_file_extension(url)
        filename = f"image_{i+1:03d}_{url_hash}{ext}"
        save_path = image_folder / filename

        print(f"  ğŸ“¥ {i+1}/{len(images)}: {filename}")

        actual_path, size = download_image(url, save_path)

        if actual_path:
            downloaded_count += 1
            stats["downloaded"] += 1
            stats["total_bytes"] += size
            stats["by_folder"][folder_name]["count"] += 1
            stats["by_folder"][folder_name]["bytes"] += size

            old_urls.append(url)
            new_paths.append(actual_path)

            print(f"    âœ… {size/1024:.1f} KB")
        else:
            stats["failed"] += 1
            new_paths.append(None)
            old_urls.append(url)

        time.sleep(REQUEST_DELAY)

    # Markdownãƒªãƒ³ã‚¯ã‚’æ›´æ–°
    if md_path and any(new_paths):
        update_markdown_links(md_path, old_urls, new_paths)

    return downloaded_count

def format_size(bytes_size):
    """ãƒã‚¤ãƒˆæ•°ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    if bytes_size < 1024:
        return f"{bytes_size} B"
    elif bytes_size < 1024 * 1024:
        return f"{bytes_size/1024:.1f} KB"
    elif bytes_size < 1024 * 1024 * 1024:
        return f"{bytes_size/(1024*1024):.1f} MB"
    else:
        return f"{bytes_size/(1024*1024*1024):.2f} GB"

def main():
    print("=" * 60)
    print("Notion API ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print(f"ä¿å­˜å…ˆ: {NOTION_IMAGES_DIR}")
    print("=" * 60)

    if not NOTION_TOKEN:
        print("âŒ NOTION_API_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # æ§‹é€ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    if not STRUCTURE_FILE.exists():
        print(f"âŒ æ§‹é€ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {STRUCTURE_FILE}")
        return

    with open(STRUCTURE_FILE, 'r', encoding='utf-8') as f:
        structure = json.load(f)

    # ãƒšãƒ¼ã‚¸ä¸€è¦§ã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯é™¤ãï¼‰
    pages = [node for node in structure.get("nodes", []) if node.get("type") == "page"]
    stats["total_pages"] = len(pages)

    print(f"\nğŸ“ å‡¦ç†å¯¾è±¡: {len(pages)} ãƒšãƒ¼ã‚¸")
    print("=" * 60)

    # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
    NOTION_IMAGES_DIR.mkdir(parents=True, exist_ok=True)

    # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
    folder_path_map = {}
    for page in pages:
        page_id = page["id"]
        path = page.get("path", [])
        folder_path_map[page_id] = path

    # å„ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
    for i, page in enumerate(pages):
        print(f"\n[{i+1}/{len(pages)}] å‡¦ç†ä¸­...", end="", flush=True)
        try:
            process_page(page, folder_path_map)
        except Exception as e:
            print(f"\n  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")

        # é€²æ—è¡¨ç¤ºï¼ˆ100ãƒšãƒ¼ã‚¸ã”ã¨ï¼‰
        if (i + 1) % 100 == 0:
            print(f"\n--- é€²æ—: {i+1}/{len(pages)} ãƒšãƒ¼ã‚¸å®Œäº†, {stats['downloaded']}ç”»åƒ, {format_size(stats['total_bytes'])} ---")

    # ãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    print(f"\nå‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {stats['total_pages']}")
    print(f"ç”»åƒã‚’å«ã‚€ãƒšãƒ¼ã‚¸: {stats['pages_with_images']}")
    print(f"åˆè¨ˆç”»åƒæ•°: {stats['total_images']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}")
    print(f"\nğŸ’¾ åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}")

    if stats["by_folder"]:
        print("\nğŸ“ ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚ºï¼ˆä¸Šä½20ä»¶ï¼‰:")
        sorted_folders = sorted(
            stats["by_folder"].items(),
            key=lambda x: x[1]["bytes"],
            reverse=True
        )[:20]

        for folder, data in sorted_folders:
            print(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}")

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_path = NOTION_IMAGES_DIR.parent / "image_download_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Notionç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ä¿å­˜å…ˆ: {NOTION_IMAGES_DIR}\n")
        f.write(f"å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {stats['total_pages']}\n")
        f.write(f"ç”»åƒã‚’å«ã‚€ãƒšãƒ¼ã‚¸: {stats['pages_with_images']}\n")
        f.write(f"åˆè¨ˆç”»åƒæ•°: {stats['total_images']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}\n")
        f.write(f"åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}\n\n")
        f.write("ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚º:\n")
        for folder, data in sorted(stats["by_folder"].items(), key=lambda x: x[1]["bytes"], reverse=True):
            f.write(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}\n")

    print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    print("\nå®Œäº†!")

if __name__ == "__main__":
    main()
