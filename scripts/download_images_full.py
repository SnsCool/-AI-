#!/usr/bin/env python3
"""
Notion APIçµŒç”±ã§å…¨ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- å…¨ãƒšãƒ¼ã‚¸ã‚’å†å¸°çš„ã«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆcolumn_list/columnå¯¾å¿œï¼‰
- ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ–°ã—ã„URLã‚’å–å¾—
- notion_images/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
- markdownã®ãƒªãƒ³ã‚¯ã‚’æ›´æ–°
"""

import urllib.request
import json
import os
import sys
import re
import time
import hashlib
from datetime import datetime
from pathlib import Path
from urllib.parse import quote, urlparse, unquote
from collections import defaultdict

# è¨­å®š
TOKEN = os.environ.get('NOTION_API_TOKEN')
ROOT_PAGE_ID = "7f19ff35-7ffc-4c78-8c71-92cb99d5204a"
BASE_DIR = Path(__file__).parent.parent
NOTION_DOCS_DIR = BASE_DIR / 'notion_docs'
NOTION_IMAGES_DIR = BASE_DIR / 'notion_images'

HEADERS = {
    "Authorization": f"Bearer {TOKEN}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json"
} if TOKEN else {}

API_DELAY = 0.35

# çµ±è¨ˆ
stats = {
    "pages_scanned": 0,
    "pages_with_images": 0,
    "total_images": 0,
    "downloaded": 0,
    "failed": 0,
    "total_bytes": 0,
    "by_folder": defaultdict(lambda: {"count": 0, "bytes": 0})
}

def api_request(url, method='GET', data=None):
    """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    req = urllib.request.Request(url, headers=HEADERS, method=method)
    if data:
        req.data = json.dumps(data).encode('utf-8')

    try:
        time.sleep(API_DELAY)
        with urllib.request.urlopen(req, timeout=30) as response:
            return json.loads(response.read().decode('utf-8'))
    except urllib.error.HTTPError as e:
        if e.code == 429:
            print("  Rate limited, waiting 30s...")
            time.sleep(30)
            return api_request(url, method, data)
        return None
    except Exception as e:
        return None

def get_block_children(block_id):
    """ãƒ–ãƒ­ãƒƒã‚¯ã®å­è¦ç´ ã‚’ã™ã¹ã¦å–å¾—"""
    all_results = []
    has_more = True
    start_cursor = None

    while has_more:
        url = f"https://api.notion.com/v1/blocks/{block_id}/children?page_size=100"
        if start_cursor:
            url += f"&start_cursor={start_cursor}"

        data = api_request(url)
        if not data:
            break

        all_results.extend(data.get('results', []))
        has_more = data.get('has_more', False)
        start_cursor = data.get('next_cursor')

    return all_results

def get_all_blocks_recursive(block_id, depth=0):
    """ãƒ–ãƒ­ãƒƒã‚¯ã¨ãã®å­å­«ã‚’ã™ã¹ã¦å†å¸°çš„ã«å–å¾—ï¼ˆcolumn_list/columnå¯¾å¿œï¼‰"""
    all_blocks = []
    blocks = get_block_children(block_id)

    for block in blocks:
        all_blocks.append(block)
        block_type = block.get("type")

        # ã“ã‚Œã‚‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ—ã¯å†å¸°çš„ã«å­ã‚’å–å¾—ï¼ˆchild_page, child_databaseã¯é™¤ãï¼‰
        if block.get("has_children") and block_type not in ["child_page", "child_database"]:
            child_blocks = get_all_blocks_recursive(block["id"], depth + 1)
            all_blocks.extend(child_blocks)

    return all_blocks

def find_child_pages(blocks):
    """ãƒ–ãƒ­ãƒƒã‚¯ãƒªã‚¹ãƒˆã‹ã‚‰child_pageã‚’æŠ½å‡º"""
    child_pages = []
    for block in blocks:
        if block.get("type") == "child_page":
            child_pages.append({
                "id": block.get("id"),
                "title": block.get("child_page", {}).get("title", "untitled")
            })
    return child_pages

def sanitize_name(name):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '_')
    return name.strip() or "unnamed"

def get_file_extension(url, content_type=None):
    """æ‹¡å¼µå­ã‚’å–å¾—"""
    parsed = urlparse(url)
    path = unquote(parsed.path)

    ext_match = re.search(r'\.([a-zA-Z0-9]+)(?:\?|$)', path)
    if ext_match:
        ext = ext_match.group(1).lower()
        if ext in ['jpg', 'jpeg', 'png', 'gif', 'webp', 'svg', 'bmp', 'ico']:
            return f".{ext}"

    if content_type:
        type_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp',
            'image/svg+xml': '.svg',
        }
        for mime, extension in type_map.items():
            if mime in content_type:
                return extension

    return '.jpg'

def download_image(url, save_path):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=60) as response:
            content = response.read()
            content_type = response.headers.get('Content-Type', '')

            expected_ext = get_file_extension(url, content_type)
            if save_path.suffix.lower() != expected_ext:
                save_path = save_path.with_suffix(expected_ext)

            save_path.parent.mkdir(parents=True, exist_ok=True)

            with open(save_path, 'wb') as f:
                f.write(content)

            return save_path, len(content)

    except Exception as e:
        print(f"    âŒ {str(e)[:60]}")
        return None, 0

def extract_images_from_blocks(blocks):
    """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ç”»åƒæƒ…å ±ã‚’æŠ½å‡º"""
    images = []

    for block in blocks:
        block_type = block.get("type")

        if block_type == "image":
            image_data = block.get("image", {})
            image_type = image_data.get("type")

            if image_type == "file":
                url = image_data.get("file", {}).get("url")
                if url:
                    images.append({"url": url, "type": "file"})
            elif image_type == "external":
                url = image_data.get("external", {}).get("url")
                if url:
                    images.append({"url": url, "type": "external"})

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆç”»åƒã®ã¿ï¼‰
        elif block_type == "file":
            file_data = block.get("file", {})
            file_type = file_data.get("type")
            if file_type == "file":
                url = file_data.get("file", {}).get("url")
                if url and any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    images.append({"url": url, "type": "file"})

    return images

def get_relative_path(image_path, md_path):
    """ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—"""
    try:
        return os.path.relpath(image_path, md_path.parent)
    except ValueError:
        return str(image_path)

def update_markdown(md_path, url_mapping):
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªãƒ³ã‚¯ã‚’æ›´æ–°"""
    if not url_mapping or not md_path.exists():
        return False

    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        modified = False
        for base_url, new_path in url_mapping.items():
            if new_path:
                # S3 URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒƒãƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿éƒ¨åˆ†ã¯é™¤ãï¼‰
                pattern = re.escape(base_url) + r'[^)\s\]"]*'
                relative_path = get_relative_path(new_path, md_path)
                if re.search(pattern, content):
                    content = re.sub(pattern, relative_path, content)
                    modified = True

        if modified:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return True
    except Exception as e:
        print(f"    âš ï¸ MDæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    return False

def process_page(page_id, page_title, folder_path):
    """1ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’å‡¦ç†"""
    stats["pages_scanned"] += 1
    print(f"\n[{stats['pages_scanned']}] ğŸ“„ {folder_path}")

    # ã“ã®ãƒšãƒ¼ã‚¸ã®ã™ã¹ã¦ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—ï¼ˆå­å­«å«ã‚€ï¼‰
    all_blocks = get_all_blocks_recursive(page_id)

    # ç”»åƒã‚’æŠ½å‡º
    images = extract_images_from_blocks(all_blocks)

    if images:
        stats["pages_with_images"] += 1
        print(f"  ğŸ“· {len(images)}ç”»åƒ")

        # ç”»åƒä¿å­˜å…ˆ
        image_folder = NOTION_IMAGES_DIR / folder_path

        # å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«
        md_path = NOTION_DOCS_DIR / folder_path / "index.md"

        url_mapping = {}

        for i, img in enumerate(images):
            stats["total_images"] += 1
            url = img["url"]

            # URLã®åŸºæœ¬éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é™¤ãï¼‰
            base_url_match = re.search(r'(https://[^?]+)', url)
            base_url = base_url_match.group(1) if base_url_match else url

            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            ext = get_file_extension(url)
            filename = f"image_{i+1:03d}_{url_hash}{ext}"
            save_path = image_folder / filename

            print(f"    [{i+1}/{len(images)}] {filename}", end=" ", flush=True)

            actual_path, size = download_image(url, save_path)

            if actual_path:
                stats["downloaded"] += 1
                stats["total_bytes"] += size
                stats["by_folder"][str(folder_path)]["count"] += 1
                stats["by_folder"][str(folder_path)]["bytes"] += size

                url_mapping[base_url] = actual_path
                print(f"âœ… {size/1024:.1f}KB")
            else:
                stats["failed"] += 1

        # Markdownãƒªãƒ³ã‚¯ã‚’æ›´æ–°
        if url_mapping:
            update_markdown(md_path, url_mapping)

    # å­ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦å†å¸°å‡¦ç†
    child_pages = find_child_pages(all_blocks)
    for child in child_pages:
        child_title = sanitize_name(child["title"])
        child_path = folder_path / child_title
        process_page(child["id"], child_title, child_path)

def format_size(bytes_size):
    """ã‚µã‚¤ã‚ºã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if bytes_size < 1024:
        return f"{bytes_size} B"
    elif bytes_size < 1024 * 1024:
        return f"{bytes_size/1024:.1f} KB"
    elif bytes_size < 1024 * 1024 * 1024:
        return f"{bytes_size/(1024*1024):.1f} MB"
    else:
        return f"{bytes_size/(1024*1024*1024):.2f} GB"

def main():
    print("=" * 60)
    print("Notion ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå…¨ãƒšãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³ï¼‰")
    print(f"ä¿å­˜å…ˆ: {NOTION_IMAGES_DIR}")
    print("=" * 60)

    if not TOKEN:
        print("âŒ NOTION_API_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
    NOTION_IMAGES_DIR.mkdir(parents=True, exist_ok=True)

    # ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å§‹
    print(f"\nğŸš€ ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å§‹: {ROOT_PAGE_ID}")

    # ãƒ«ãƒ¼ãƒˆãƒšãƒ¼ã‚¸ã®ã™ã¹ã¦ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—ï¼ˆcolumn_list/columnå«ã‚€ï¼‰
    all_root_blocks = get_all_blocks_recursive(ROOT_PAGE_ID)

    # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®å­ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    child_pages = find_child_pages(all_root_blocks)
    print(f"\nğŸ“ ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒšãƒ¼ã‚¸æ•°: {len(child_pages)}")

    for child in child_pages:
        child_title = sanitize_name(child["title"])
        process_page(child["id"], child_title, Path(child_title))

        # 100ãƒšãƒ¼ã‚¸ã”ã¨ã«é€²æ—è¡¨ç¤º
        if stats["pages_scanned"] % 100 == 0:
            print(f"\n--- é€²æ—: {stats['pages_scanned']}ãƒšãƒ¼ã‚¸, {stats['downloaded']}ç”»åƒ, {format_size(stats['total_bytes'])} ---")

    # ãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š çµæœãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    print(f"\nã‚¹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ã‚¸æ•°: {stats['pages_scanned']}")
    print(f"ç”»åƒã‚’å«ã‚€ãƒšãƒ¼ã‚¸: {stats['pages_with_images']}")
    print(f"åˆè¨ˆç”»åƒæ•°: {stats['total_images']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}")
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}")
    print(f"\nğŸ’¾ åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}")

    if stats["by_folder"]:
        print("\nğŸ“ ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚ºï¼ˆä¸Šä½20ä»¶ï¼‰:")
        sorted_folders = sorted(
            stats["by_folder"].items(),
            key=lambda x: x[1]["bytes"],
            reverse=True
        )[:20]
        for folder, data in sorted_folders:
            print(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}")

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_path = BASE_DIR / "image_download_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Notionç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ä¿å­˜å…ˆ: {NOTION_IMAGES_DIR}\n")
        f.write(f"ã‚¹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ã‚¸æ•°: {stats['pages_scanned']}\n")
        f.write(f"ç”»åƒã‚’å«ã‚€ãƒšãƒ¼ã‚¸: {stats['pages_with_images']}\n")
        f.write(f"åˆè¨ˆç”»åƒæ•°: {stats['total_images']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {stats['downloaded']}\n")
        f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {stats['failed']}\n")
        f.write(f"åˆè¨ˆã‚µã‚¤ã‚º: {format_size(stats['total_bytes'])}\n\n")
        f.write("ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã‚µã‚¤ã‚º:\n")
        for folder, data in sorted(stats["by_folder"].items(), key=lambda x: x[1]["bytes"], reverse=True):
            f.write(f"  {folder}: {data['count']}ç”»åƒ, {format_size(data['bytes'])}\n")

    print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print("\nâœ… å®Œäº†!")

if __name__ == "__main__":
    main()
