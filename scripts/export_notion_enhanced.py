#!/usr/bin/env python3
"""
Notionå®Œå…¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ - æ‹¡å¼µç‰ˆ
- ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã®ä¸­èº«ã‚’å–å¾—
- ãƒªãƒ³ã‚¯å…ˆã®Webãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- å‹•ç”»ãƒ»éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + æ–‡å­—èµ·ã“ã—
- PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
"""

import urllib.request
import json
import os
import sys
import re
import time
import hashlib
import subprocess
from datetime import datetime
from pathlib import Path
from urllib.parse import quote, urlparse, urljoin
from html.parser import HTMLParser
import ssl

# SSLè¨¼æ˜æ›¸æ¤œè¨¼ã‚’ç„¡åŠ¹åŒ–ï¼ˆä¸€éƒ¨ã‚µã‚¤ãƒˆå¯¾å¿œï¼‰
ssl._create_default_https_context = ssl._create_unverified_context

# è¨­å®š
TOKEN = os.environ.get('NOTION_API_TOKEN')
ROOT_PAGE_ID = "7f19ff35-7ffc-4c78-8c71-92cb99d5204a"
BASE_DIR = Path(__file__).parent.parent
OUTPUT_DIR = BASE_DIR / 'notion_docs'
MEDIA_DIR = BASE_DIR / 'notion_media'
IMAGES_DIR = BASE_DIR / 'notion_images'
TRANSCRIPTS_DIR = BASE_DIR / 'notion_transcripts'

HEADERS = {
    "Authorization": f"Bearer {TOKEN}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json"
} if TOKEN else {}

# APIå‘¼ã³å‡ºã—é–“éš”
API_DELAY = 0.35

# çµ±è¨ˆ
stats = {
    'pages': 0,
    'databases': 0,
    'records': 0,
    'images': 0,
    'tables': 0,
    'links_scraped': 0,
    'media_downloaded': 0,
    'transcripts': 0,
    'pdfs': 0,
    'errors': []
}

# HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºç”¨
class HTMLTextExtractor(HTMLParser):
    def __init__(self):
        super().__init__()
        self.text = []
        self.skip_tags = {'script', 'style', 'nav', 'header', 'footer', 'aside'}
        self.current_tag = None

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag

    def handle_endtag(self, tag):
        if tag in ['p', 'div', 'br', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            self.text.append('\n')
        self.current_tag = None

    def handle_data(self, data):
        if self.current_tag not in self.skip_tags:
            text = data.strip()
            if text:
                self.text.append(text + ' ')

    def get_text(self):
        return ''.join(self.text).strip()

def api_request(url, method='GET', data=None):
    """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    req = urllib.request.Request(url, headers=HEADERS, method=method)
    if data:
        req.data = json.dumps(data).encode('utf-8')
    try:
        time.sleep(API_DELAY)
        with urllib.request.urlopen(req, timeout=30) as response:
            return json.loads(response.read().decode('utf-8'))
    except urllib.error.HTTPError as e:
        if e.code == 429:
            print("  Rate limited, waiting 30s...")
            time.sleep(30)
            return api_request(url, method, data)
        stats['errors'].append(f"HTTP {e.code}: {url}")
        return None
    except Exception as e:
        stats['errors'].append(f"Error: {e}")
        return None

def get_block_children(block_id):
    """ãƒ–ãƒ­ãƒƒã‚¯ã®å­è¦ç´ ã‚’ã™ã¹ã¦å–å¾—"""
    all_results = []
    has_more = True
    start_cursor = None
    while has_more:
        url = f"https://api.notion.com/v1/blocks/{block_id}/children?page_size=100"
        if start_cursor:
            url += f"&start_cursor={start_cursor}"
        data = api_request(url)
        if not data:
            break
        all_results.extend(data.get('results', []))
        has_more = data.get('has_more', False)
        start_cursor = data.get('next_cursor')
    return all_results

def download_file(url, save_dir, prefix=""):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        parsed = urlparse(url)
        path = parsed.path
        # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        from urllib.parse import unquote
        decoded_path = unquote(path)
        ext = os.path.splitext(decoded_path)[1] or '.bin'
        # æ‹¡å¼µå­ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        ext = ext.split('?')[0]
        filename = f"{prefix}{url_hash}{ext}"
        filepath = save_dir / filename

        if filepath.exists():
            return str(filepath), filename

        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0')

        with urllib.request.urlopen(req, timeout=120) as response:
            with open(filepath, 'wb') as f:
                f.write(response.read())

        return str(filepath), filename
    except Exception as e:
        stats['errors'].append(f"Download error: {e}")
        return None, None

def scrape_webpage(url):
    """Webãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)')

        with urllib.request.urlopen(req, timeout=15) as response:
            html = response.read().decode('utf-8', errors='ignore')

        # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        extractor = HTMLTextExtractor()
        extractor.feed(html)
        text = extractor.get_text()

        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > 5000:
            text = text[:5000] + "\n\n[... ä»¥ä¸‹çœç•¥ ...]"

        stats['links_scraped'] += 1
        return text
    except Exception as e:
        return f"[ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}]"

def transcribe_audio(filepath):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆGemini 2.0 Flashå„ªå…ˆï¼‰"""
    # Gemini APIã‚’å„ªå…ˆä½¿ç”¨
    return transcribe_with_gemini(filepath)

def transcribe_with_gemini(filepath):
    """Gemini 2.0 Flashã§æ–‡å­—èµ·ã“ã—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰"""
    try:
        import google.generativeai as genai

        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            return "[æ–‡å­—èµ·ã“ã—ã‚¹ã‚­ãƒƒãƒ—: GEMINI_API_KEYæœªè¨­å®š]"

        genai.configure(api_key=api_key)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"      ğŸ“¤ Geminiã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        uploaded_file = genai.upload_file(filepath)

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…ã¤
        import time
        while uploaded_file.state.name == "PROCESSING":
            time.sleep(2)
            uploaded_file = genai.get_file(uploaded_file.name)

        if uploaded_file.state.name == "FAILED":
            return f"[æ–‡å­—èµ·ã“ã—å¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—]"

        # Gemini 2.0 Flashã§æ–‡å­—èµ·ã“ã—
        model = genai.GenerativeModel('gemini-2.0-flash')

        prompt = """ã“ã®éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®å½¢å¼ã§ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

[MM:SS] ç™ºè¨€å†…å®¹

ä¾‹ï¼š
[00:00] ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯...
[00:15] ãã‚Œã§ã¯å§‹ã‚ã¾ã—ã‚‡ã†

æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        response = model.generate_content([prompt, uploaded_file])

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            genai.delete_file(uploaded_file.name)
        except:
            pass

        stats['transcripts'] += 1
        return response.text

    except Exception as e:
        return f"[æ–‡å­—èµ·ã“ã—å¤±æ•—: {e}]"

def transcribe_with_openai(filepath):
    """OpenAI Whisper APIã§æ–‡å­—èµ·ã“ã—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        import openai

        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            # Geminiã‚’è©¦ã™
            return transcribe_with_gemini(filepath)

        client = openai.OpenAI(api_key=api_key)

        with open(filepath, 'rb') as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",
                response_format="text"
            )

        stats['transcripts'] += 1
        return transcript
    except Exception as e:
        return f"[æ–‡å­—èµ·ã“ã—å¤±æ•—: {e}]"

def extract_pdf_text(filepath):
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    try:
        # PyPDF2ã‚’è©¦ã™
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(filepath)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            if text.strip():
                stats['pdfs'] += 1
                return text
        except ImportError:
            pass

        # pdfplumberã‚’è©¦ã™
        try:
            import pdfplumber
            with pdfplumber.open(filepath) as pdf:
                text = ""
                for page in pdf.pages:
                    text += (page.extract_text() or "") + "\n"
            if text.strip():
                stats['pdfs'] += 1
                return text
        except ImportError:
            pass

        return "[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (pip install PyPDF2)]"

    except Exception as e:
        return f"[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: {e}]"

def table_to_markdown(block_id):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã‚’Markdownã«å¤‰æ›"""
    try:
        rows = get_block_children(block_id)
        if not rows:
            return "[ãƒ†ãƒ¼ãƒ–ãƒ«: ãƒ‡ãƒ¼ã‚¿ãªã—]\n"

        md_lines = []
        for i, row in enumerate(rows):
            if row.get('type') != 'table_row':
                continue
            cells = row.get('table_row', {}).get('cells', [])
            row_text = []
            for cell in cells:
                cell_text = ''.join([t.get('plain_text', '') for t in cell])
                cell_text = cell_text.replace('|', '\\|').replace('\n', ' ')
                row_text.append(cell_text)
            md_lines.append("| " + " | ".join(row_text) + " |")

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®å¾Œã«ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿
            if i == 0:
                md_lines.append("|" + "---|" * len(row_text))

        stats['tables'] += 1
        return "\n".join(md_lines) + "\n"
    except Exception as e:
        return f"[ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—å¤±æ•—: {e}]\n"

def rich_text_to_markdown(rich_text_array):
    """Notion rich_text ã‚’ Markdown ã«å¤‰æ›"""
    if not rich_text_array:
        return ""
    result = []
    for rt in rich_text_array:
        text = rt.get('plain_text', '')
        annotations = rt.get('annotations', {})
        if annotations.get('bold'):
            text = f"**{text}**"
        if annotations.get('italic'):
            text = f"*{text}*"
        if annotations.get('strikethrough'):
            text = f"~~{text}~~"
        if annotations.get('code'):
            text = f"`{text}`"
        href = rt.get('href')
        if href:
            text = f"[{text}]({href})"
        result.append(text)
    return ''.join(result)

def get_database_info(database_id):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—"""
    url = f"https://api.notion.com/v1/databases/{database_id}"
    return api_request(url)

def query_database(database_id):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
    all_results = []
    has_more = True
    start_cursor = None
    while has_more:
        url = f"https://api.notion.com/v1/databases/{database_id}/query"
        data = {"page_size": 100}
        if start_cursor:
            data["start_cursor"] = start_cursor
        result = api_request(url, method='POST', data=data)
        if not result:
            break
        all_results.extend(result.get('results', []))
        has_more = result.get('has_more', False)
        start_cursor = result.get('next_cursor')
    return all_results

def property_value_to_string(prop):
    """Notionãƒ—ãƒ­ãƒ‘ãƒ†ã‚£å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›"""
    prop_type = prop.get('type', '')
    if prop_type == 'title':
        return rich_text_to_markdown(prop.get('title', []))
    elif prop_type == 'rich_text':
        return rich_text_to_markdown(prop.get('rich_text', []))
    elif prop_type == 'number':
        val = prop.get('number')
        return str(val) if val is not None else ''
    elif prop_type == 'select':
        select = prop.get('select')
        return select.get('name', '') if select else ''
    elif prop_type == 'multi_select':
        return ', '.join([s.get('name', '') for s in prop.get('multi_select', [])])
    elif prop_type == 'date':
        date = prop.get('date')
        if date:
            start = date.get('start', '')
            end = date.get('end', '')
            return f"{start} â†’ {end}" if end else start
        return ''
    elif prop_type == 'people':
        return ', '.join([p.get('name', '') for p in prop.get('people', [])])
    elif prop_type == 'files':
        files = prop.get('files', [])
        return ', '.join([f.get('name', '') or f.get('file', {}).get('url', '') for f in files])
    elif prop_type == 'checkbox':
        return 'âœ“' if prop.get('checkbox') else 'âœ—'
    elif prop_type == 'url':
        return prop.get('url', '') or ''
    elif prop_type == 'email':
        return prop.get('email', '') or ''
    elif prop_type == 'phone_number':
        return prop.get('phone_number', '') or ''
    elif prop_type == 'status':
        status = prop.get('status')
        return status.get('name', '') if status else ''
    elif not prop_type:
        return ''
    else:
        return f"[{prop_type}]"

def database_to_markdown(database_id, title):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å…¨æƒ…å ±ã‚’Markdownã«å¤‰æ›"""
    md_parts = [f"\n### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {title}\n\n"]
    db_info = get_database_info(database_id)
    if not db_info:
        return f"\n### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {title}\n\n(ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ)\n"

    properties = db_info.get('properties', {})
    if properties:
        md_parts.append("#### ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆã‚«ãƒ©ãƒ ï¼‰\n\n")
        md_parts.append("| ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£å | ã‚¿ã‚¤ãƒ— |\n")
        md_parts.append("|------------|--------|\n")
        for prop_name, prop_info in properties.items():
            prop_type = prop_info.get('type', 'unknown')
            md_parts.append(f"| {prop_name} | {prop_type} |\n")
        md_parts.append("\n")

    records = query_database(database_id)
    stats['records'] += len(records)

    if not records:
        md_parts.append("(ãƒ¬ã‚³ãƒ¼ãƒ‰ãªã—)\n")
        return ''.join(md_parts)

    md_parts.append(f"#### ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆ{len(records)}ä»¶ï¼‰\n\n")

    prop_names = list(properties.keys())
    title_prop = None
    for name, info in properties.items():
        if info.get('type') == 'title':
            title_prop = name
            break
    if title_prop:
        prop_names.remove(title_prop)
        prop_names.insert(0, title_prop)

    md_parts.append("| " + " | ".join(prop_names[:8]) + " |\n")
    md_parts.append("|" + "---|" * min(len(prop_names), 8) + "\n")

    for record in records[:100]:
        record_props = record.get('properties', {})
        row = []
        for prop_name in prop_names[:8]:
            prop = record_props.get(prop_name, {})
            value = property_value_to_string(prop)
            if value is None:
                value = ''
            value = str(value).replace('|', '\\|').replace('\n', ' ')[:50]
            row.append(value)
        md_parts.append("| " + " | ".join(row) + " |\n")

    if len(records) > 100:
        md_parts.append(f"\n*ï¼ˆä»– {len(records) - 100} ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰*\n")

    return ''.join(md_parts)

def block_to_markdown(block, indent_level=0):
    """Notionãƒ–ãƒ­ãƒƒã‚¯ã‚’ Markdown ã«å¤‰æ›ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    block_type = block.get('type', '')
    block_id = block.get('id', '')
    indent = "  " * indent_level

    if block_type == 'paragraph':
        text = rich_text_to_markdown(block.get('paragraph', {}).get('rich_text', []))
        return f"{indent}{text}\n" if text else "\n"

    elif block_type == 'heading_1':
        text = rich_text_to_markdown(block.get('heading_1', {}).get('rich_text', []))
        return f"{indent}# {text}\n"

    elif block_type == 'heading_2':
        text = rich_text_to_markdown(block.get('heading_2', {}).get('rich_text', []))
        return f"{indent}## {text}\n"

    elif block_type == 'heading_3':
        text = rich_text_to_markdown(block.get('heading_3', {}).get('rich_text', []))
        return f"{indent}### {text}\n"

    elif block_type == 'bulleted_list_item':
        text = rich_text_to_markdown(block.get('bulleted_list_item', {}).get('rich_text', []))
        return f"{indent}- {text}\n"

    elif block_type == 'numbered_list_item':
        text = rich_text_to_markdown(block.get('numbered_list_item', {}).get('rich_text', []))
        return f"{indent}1. {text}\n"

    elif block_type == 'to_do':
        todo = block.get('to_do', {})
        text = rich_text_to_markdown(todo.get('rich_text', []))
        checked = "x" if todo.get('checked') else " "
        return f"{indent}- [{checked}] {text}\n"

    elif block_type == 'toggle':
        text = rich_text_to_markdown(block.get('toggle', {}).get('rich_text', []))
        return f"{indent}<details>\n{indent}<summary>{text}</summary>\n"

    elif block_type == 'code':
        code = block.get('code', {})
        text = rich_text_to_markdown(code.get('rich_text', []))
        lang = code.get('language', '')
        return f"{indent}```{lang}\n{text}\n{indent}```\n"

    elif block_type == 'quote':
        text = rich_text_to_markdown(block.get('quote', {}).get('rich_text', []))
        return f"{indent}> {text}\n"

    elif block_type == 'callout':
        callout = block.get('callout', {})
        text = rich_text_to_markdown(callout.get('rich_text', []))
        icon_data = callout.get('icon') or {}
        icon = icon_data.get('emoji', 'ğŸ’¡') if isinstance(icon_data, dict) else 'ğŸ’¡'
        return f"{indent}> {icon} {text}\n"

    elif block_type == 'divider':
        return f"{indent}---\n"

    elif block_type == 'table':
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¸­èº«ã‚’å–å¾—
        return f"{indent}{table_to_markdown(block_id)}\n"

    elif block_type == 'image':
        image = block.get('image', {})
        url = image.get('file', {}).get('url') or image.get('external', {}).get('url', '')
        caption = rich_text_to_markdown(image.get('caption', []))
        if url and 'amazonaws.com' in url:
            filepath, filename = download_file(url, IMAGES_DIR, "img_")
            if filepath:
                stats['images'] += 1
                url = f"../../../notion_images/{filename}"
        return f"{indent}![{caption}]({url})\n"

    elif block_type == 'bookmark':
        url = block.get('bookmark', {}).get('url', '')
        # ãƒªãƒ³ã‚¯å…ˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        content = ""
        if url and not any(x in url for x in ['youtube.com', 'youtu.be', 'twitter.com', 'x.com']):
            print(f"    ğŸ”— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {url[:50]}...")
            scraped = scrape_webpage(url)
            if scraped and not scraped.startswith('['):
                content = f"\n\n<details>\n<summary>ãƒªãƒ³ã‚¯å…ˆã®å†…å®¹</summary>\n\n{scraped}\n\n</details>\n"
        return f"{indent}[Bookmark: {url}]({url}){content}\n"

    elif block_type == 'link_preview':
        url = block.get('link_preview', {}).get('url', '')
        return f"{indent}[Link: {url}]({url})\n"

    elif block_type == 'child_page':
        title = block.get('child_page', {}).get('title', 'Untitled')
        return f"{indent}ğŸ“„ **{title}** (å­ãƒšãƒ¼ã‚¸)\n"

    elif block_type == 'child_database':
        title = block.get('child_database', {}).get('title', 'Untitled')
        db_content = database_to_markdown(block_id, title)
        return f"{indent}ğŸ“Š **{title}** (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)\n{db_content}\n"

    elif block_type == 'embed':
        url = block.get('embed', {}).get('url', '')
        return f"{indent}[Embed: {url}]({url})\n"

    elif block_type == 'video':
        video = block.get('video', {})
        url = video.get('file', {}).get('url') or video.get('external', {}).get('url', '')

        result = f"{indent}[Video: {url}]({url})\n"

        # å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + æ–‡å­—èµ·ã“ã—
        if url and 'amazonaws.com' in url:
            print(f"    ğŸ¬ å‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            filepath, filename = download_file(url, MEDIA_DIR, "video_")
            if filepath:
                stats['media_downloaded'] += 1
                result = f"{indent}[Video: notion_media/{filename}](../../../notion_media/{filename})\n"

                # æ–‡å­—èµ·ã“ã—
                print(f"    ğŸ“ æ–‡å­—èµ·ã“ã—ä¸­...")
                transcript = transcribe_audio(filepath)
                if transcript and not transcript.startswith('['):
                    result += f"\n<details>\n<summary>ğŸ“ æ–‡å­—èµ·ã“ã—</summary>\n\n{transcript}\n\n</details>\n"

        return result

    elif block_type == 'audio':
        audio = block.get('audio', {})
        url = audio.get('file', {}).get('url') or audio.get('external', {}).get('url', '')

        result = f"{indent}[Audio: {url}]({url})\n"

        if url and 'amazonaws.com' in url:
            print(f"    ğŸµ éŸ³å£°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            filepath, filename = download_file(url, MEDIA_DIR, "audio_")
            if filepath:
                stats['media_downloaded'] += 1
                result = f"{indent}[Audio: notion_media/{filename}](../../../notion_media/{filename})\n"

                print(f"    ğŸ“ æ–‡å­—èµ·ã“ã—ä¸­...")
                transcript = transcribe_audio(filepath)
                if transcript and not transcript.startswith('['):
                    result += f"\n<details>\n<summary>ğŸ“ æ–‡å­—èµ·ã“ã—</summary>\n\n{transcript}\n\n</details>\n"

        return result

    elif block_type == 'pdf':
        pdf = block.get('pdf', {})
        url = pdf.get('file', {}).get('url') or pdf.get('external', {}).get('url', '')

        result = f"{indent}[PDF: {url}]({url})\n"

        if url and 'amazonaws.com' in url:
            print(f"    ğŸ“„ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            filepath, filename = download_file(url, MEDIA_DIR, "pdf_")
            if filepath:
                stats['media_downloaded'] += 1
                result = f"{indent}[PDF: notion_media/{filename}](../../../notion_media/{filename})\n"

                print(f"    ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­...")
                text = extract_pdf_text(filepath)
                if text and not text.startswith('['):
                    result += f"\n<details>\n<summary>ğŸ“ PDFãƒ†ã‚­ã‚¹ãƒˆ</summary>\n\n{text[:3000]}\n\n</details>\n"

        return result

    elif block_type == 'file':
        file_data = block.get('file', {})
        url = file_data.get('file', {}).get('url') or file_data.get('external', {}).get('url', '')

        if url and 'amazonaws.com' in url:
            print(f"    ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            filepath, filename = download_file(url, MEDIA_DIR, "file_")
            if filepath:
                stats['media_downloaded'] += 1
                return f"{indent}[File: notion_media/{filename}](../../../notion_media/{filename})\n"

        return f"{indent}[File: {url}]({url})\n"

    elif block_type in ['column_list', 'column', 'synced_block']:
        return ""

    else:
        return ""

def fetch_page_content(page_id, max_depth=5, current_depth=0):
    """ãƒšãƒ¼ã‚¸ã®å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦Markdownã«å¤‰æ›"""
    if current_depth > max_depth:
        return ""
    blocks = get_block_children(page_id)
    markdown_parts = []
    for block in blocks:
        md = block_to_markdown(block)
        if md:
            markdown_parts.append(md)
        if block.get('has_children', False):
            block_type = block.get('type', '')
            if block_type not in ['child_page', 'child_database']:
                child_content = fetch_page_content(block['id'], max_depth, current_depth + 1)
                if child_content:
                    markdown_parts.append(child_content)
    return ''.join(markdown_parts)

def sanitize_filename(name):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›"""
    unsafe_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|', '\n', '\r']
    result = name
    for char in unsafe_chars:
        result = result.replace(char, '_')
    result = result.strip('. ')
    if len(result) > 100:
        result = result[:100]
    return result or 'unnamed'

def fetch_hierarchy_with_content(page_id, depth=0, max_depth=7, visited=None, parent_path=""):
    """éšå±¤æ§‹é€ ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å†å¸°çš„ã«å–å¾—"""
    if visited is None:
        visited = set()
    if depth > max_depth or page_id in visited:
        return []
    visited.add(page_id)
    blocks = get_block_children(page_id)
    results = []

    for block in blocks:
        block_type = block.get('type', '')
        block_id = block['id']
        has_children = block.get('has_children', False)
        node = None

        if block_type == 'child_page':
            title = block.get('child_page', {}).get('title', 'Untitled')
            print(f"{'  ' * depth}ğŸ“„ {title}", flush=True)
            stats['pages'] += 1
            content = fetch_page_content(block_id)
            node = {
                'id': block_id,
                'name': title.strip(),
                'type': 'page',
                'depth': depth,
                'content': content,
                'path': f"{parent_path}/{sanitize_filename(title)}" if parent_path else sanitize_filename(title),
                'children': []
            }
            if has_children:
                child_path = node['path']
                children = fetch_hierarchy_with_content(block_id, depth + 1, max_depth, visited.copy(), child_path)
                node['children'] = children

        elif block_type == 'child_database':
            title = block.get('child_database', {}).get('title', 'Untitled')
            print(f"{'  ' * depth}ğŸ“Š {title}", flush=True)
            stats['databases'] += 1
            db_content = database_to_markdown(block_id, title)
            node = {
                'id': block_id,
                'name': title.strip(),
                'type': 'database',
                'depth': depth,
                'content': db_content,
                'path': f"{parent_path}/{sanitize_filename(title)}" if parent_path else sanitize_filename(title),
                'children': []
            }

        elif block_type in ['column_list', 'column', 'toggle', 'synced_block', 'callout'] and has_children:
            children = fetch_hierarchy_with_content(block_id, depth, max_depth, visited.copy(), parent_path)
            results.extend(children)
            continue

        if node:
            results.append(node)
    return results

def generate_index_md(node, all_descendants=None):
    """ãƒãƒ¼ãƒ‰ã®index.mdã‚’ç”Ÿæˆ"""
    now = datetime.now().strftime('%Y-%m-%d %H:%M')
    md = f"""# {node['name']}

**ç¨®é¡**: {'ğŸ“„ ãƒšãƒ¼ã‚¸' if node['type'] == 'page' else 'ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹'}
**éšå±¤**: {node['depth'] + 1}
**æ›´æ–°æ—¥æ™‚**: {now}

---

## ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

{node.get('content', '(ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãªã—)')}

---

## å­è¦ç´ ä¸€è¦§

"""
    if node['children']:
        for child in node['children']:
            icon = 'ğŸ“Š' if child['type'] == 'database' else 'ğŸ“„'
            md += f"- {icon} [{child['name']}](./{sanitize_filename(child['name'])}/index.md)\n"
    else:
        md += "(å­è¦ç´ ãªã—)\n"

    if all_descendants:
        md += f"""
---

## å…¨å­å­«æ§‹é€ 

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³é…ä¸‹ã®å…¨ãƒšãƒ¼ã‚¸/ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ{len(all_descendants)}ä»¶ï¼‰:

"""
        for desc in all_descendants:
            indent = "  " * desc['depth']
            icon = 'ğŸ“Š' if desc['type'] == 'database' else 'ğŸ“„'
            md += f"{indent}- {icon} {desc['name']}\n"
    md += f"\n---\n*Generated: {now}*\n"
    return md

def get_all_descendants(node):
    """ãƒãƒ¼ãƒ‰ã®å…¨å­å­«ã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã§å–å¾—"""
    descendants = []
    for child in node.get('children', []):
        descendants.append({
            'name': child['name'],
            'type': child['type'],
            'depth': child['depth'] - node['depth']
        })
        descendants.extend([
            {**d, 'depth': d['depth'] + 1}
            for d in get_all_descendants(child)
        ])
    return descendants

def create_folder_structure(nodes, base_path):
    """ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œæˆ"""
    for node in nodes:
        folder_path = base_path / sanitize_filename(node['name'])
        folder_path.mkdir(parents=True, exist_ok=True)
        descendants = get_all_descendants(node)
        index_content = generate_index_md(node, descendants)
        index_file = folder_path / 'index.md'
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(index_content)
        if node['children']:
            create_folder_structure(node['children'], folder_path)

def main():
    print("=" * 60, flush=True)
    print("Notionå®Œå…¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ - æ‹¡å¼µç‰ˆ", flush=True)
    print("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªãƒ³ã‚¯ãƒ»å‹•ç”»ãƒ»éŸ³å£°ãƒ»PDFå¯¾å¿œ", flush=True)
    print("=" * 60, flush=True)
    print(flush=True)

    if not TOKEN:
        print("âŒ NOTION_API_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™
    if OUTPUT_DIR.exists():
        print(f"ğŸ“ æ—¢å­˜ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤: {OUTPUT_DIR}", flush=True)
        import shutil
        shutil.rmtree(OUTPUT_DIR)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    MEDIA_DIR.mkdir(parents=True, exist_ok=True)
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    TRANSCRIPTS_DIR.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {OUTPUT_DIR}", flush=True)
    print(f"ğŸ“ ãƒ¡ãƒ‡ã‚£ã‚¢ä¿å­˜å…ˆ: {MEDIA_DIR}", flush=True)
    print(f"ğŸ“ ç”»åƒä¿å­˜å…ˆ: {IMAGES_DIR}", flush=True)
    print(flush=True)

    print("ğŸŒ Notionã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...", flush=True)
    print("   (ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ãƒªãƒ³ã‚¯ãƒ»å‹•ç”»ã‚‚å‡¦ç†ã™ã‚‹ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)", flush=True)
    print(flush=True)

    hierarchy = fetch_hierarchy_with_content(ROOT_PAGE_ID)

    print()
    print(f"âœ… å–å¾—å®Œäº†!")
    print(f"   - ãƒšãƒ¼ã‚¸: {stats['pages']} ä»¶")
    print(f"   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {stats['databases']} ä»¶")
    print(f"   - ãƒ¬ã‚³ãƒ¼ãƒ‰: {stats['records']} ä»¶")
    print(f"   - ãƒ†ãƒ¼ãƒ–ãƒ«: {stats['tables']} ä»¶")
    print(f"   - ç”»åƒ: {stats['images']} ä»¶")
    print(f"   - ãƒªãƒ³ã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {stats['links_scraped']} ä»¶")
    print(f"   - ãƒ¡ãƒ‡ã‚£ã‚¢DL: {stats['media_downloaded']} ä»¶")
    print(f"   - æ–‡å­—èµ·ã“ã—: {stats['transcripts']} ä»¶")
    print(f"   - PDFãƒ†ã‚­ã‚¹ãƒˆ: {stats['pdfs']} ä»¶")
    if stats['errors']:
        print(f"   - ã‚¨ãƒ©ãƒ¼: {len(stats['errors'])} ä»¶")
    print()

    print("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œæˆä¸­...")
    create_folder_structure(hierarchy, OUTPUT_DIR)

    # ãƒ«ãƒ¼ãƒˆã®index.mdã‚’ä½œæˆ
    root_index = f"""# Levela Portal ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆå®Œå…¨ç‰ˆãƒ»æ‹¡å¼µï¼‰

**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

## çµ±è¨ˆ

- ãƒšãƒ¼ã‚¸æ•°: {stats['pages']}
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•°: {stats['databases']}
- ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['records']}
- ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {stats['tables']}
- ç”»åƒæ•°: {stats['images']}
- ãƒªãƒ³ã‚¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {stats['links_scraped']}
- ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {stats['media_downloaded']}
- æ–‡å­—èµ·ã“ã—: {stats['transcripts']}
- PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: {stats['pdfs']}

---

## ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§

"""
    for node in hierarchy:
        icon = 'ğŸ“Š' if node['type'] == 'database' else 'ğŸ“„'
        root_index += f"- {icon} [{node['name']}](./{sanitize_filename(node['name'])}/index.md)\n"

    root_index += f"""
---

## ä½¿ã„æ–¹

1. å„ãƒ•ã‚©ãƒ«ãƒ€ã¯Notionã®éšå±¤æ§‹é€ ã«å¯¾å¿œã—ã¦ã„ã¾ã™
2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆã‚«ãƒ©ãƒ ï¼‰ã¨ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆè¡Œï¼‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™
3. ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¸­èº«ã‚‚Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦å‡ºåŠ›ã•ã‚Œã¦ã„ã¾ã™
4. ãƒªãƒ³ã‚¯å…ˆã®Webãƒšãƒ¼ã‚¸å†…å®¹ãŒ <details> ã‚¿ã‚°å†…ã«æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™
5. å‹•ç”»ãƒ»éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ notion_media/ ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€æ–‡å­—èµ·ã“ã—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™
6. PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã™

---

*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""

    with open(OUTPUT_DIR / 'index.md', 'w', encoding='utf-8') as f:
        f.write(root_index)

    print()
    print("=" * 60)
    print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†!")
    print(f"   å‡ºåŠ›å…ˆ: {OUTPUT_DIR}")
    print(f"   ãƒ¡ãƒ‡ã‚£ã‚¢: {MEDIA_DIR}")
    print(f"   ç”»åƒ: {IMAGES_DIR}")
    print("=" * 60)

if __name__ == "__main__":
    main()
