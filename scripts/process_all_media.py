#!/usr/bin/env python3
"""
Notion docså†…ã®å…¨å‹•ç”»ãƒ»PDFã‚’å‡¦ç†ã—ã¦ãƒŠãƒ¬ãƒƒã‚¸åŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å‡¦ç†çµæœã¯ knowledge_base/ ãƒ•ã‚©ãƒ«ãƒ€ã«æ ¼ç´ï¼ˆnotion_docsã¯å¤‰æ›´ã—ãªã„ï¼‰
"""

import os
import re
import json
import hashlib
import urllib.request
import tempfile
import time
from pathlib import Path
from datetime import datetime

# è¨­å®š
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
BASE_DIR = Path(__file__).parent.parent
NOTION_DOCS = BASE_DIR / 'notion_docs'
KNOWLEDGE_BASE = BASE_DIR / 'knowledge_base'

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
TRANSCRIPTS_DIR = KNOWLEDGE_BASE / 'transcripts'
PDF_TEXTS_DIR = KNOWLEDGE_BASE / 'pdf_texts'
LINK_CONTENTS_DIR = KNOWLEDGE_BASE / 'link_contents'

# å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è·¡
processed_log = KNOWLEDGE_BASE / 'processed_media.json'

def ensure_dirs():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    TRANSCRIPTS_DIR.mkdir(parents=True, exist_ok=True)
    PDF_TEXTS_DIR.mkdir(parents=True, exist_ok=True)
    LINK_CONTENTS_DIR.mkdir(parents=True, exist_ok=True)

def load_processed():
    """å‡¦ç†æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if processed_log.exists():
        with open(processed_log, 'r') as f:
            return json.load(f)
    return {'loom': [], 'youtube': [], 'pdf': [], 'audio': []}

def save_processed(data):
    """å‡¦ç†æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’ä¿å­˜"""
    with open(processed_log, 'w') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def get_file_hash(url):
    """URLã‹ã‚‰ãƒãƒƒã‚·ãƒ¥IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(url.encode()).hexdigest()[:12]

# ============ Loomå‡¦ç† ============

def get_loom_info(video_id):
    """Loomå‹•ç”»ã®æƒ…å ±ã‚’å–å¾—"""
    url = f"https://www.loom.com/share/{video_id}"

    try:
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        with urllib.request.urlopen(req, timeout=30) as response:
            html = response.read().decode('utf-8')

        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        title_match = re.search(r'<title>([^<]+)</title>', html)
        title = title_match.group(1).replace(' | Loom', '').strip() if title_match else "Loom Video"

        return {
            'title': title,
            'url': url
        }

    except Exception as e:
        print(f"  Loomå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def process_loom_videos():
    """å…¨Loomå‹•ç”»ã‚’å‡¦ç†"""
    print("\n" + "="*60)
    print("Loomå‹•ç”»ã®æƒ…å ±åé›†")
    print("="*60)

    processed = load_processed()

    # notion_docså†…ã®Loomãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
    loom_pattern = re.compile(r'loom\.com/share/([a-z0-9]+)')

    loom_videos = {}
    for md_file in NOTION_DOCS.rglob('*.md'):
        content = md_file.read_text(encoding='utf-8')
        for match in loom_pattern.finditer(content):
            video_id = match.group(1)
            if video_id not in loom_videos:
                loom_videos[video_id] = []
            rel_path = str(md_file.relative_to(NOTION_DOCS))
            if rel_path not in loom_videos[video_id]:
                loom_videos[video_id].append(rel_path)

    print(f"\nç™ºè¦‹ã—ãŸLoomå‹•ç”»: {len(loom_videos)}ä»¶")

    new_count = 0
    for video_id, source_files in loom_videos.items():
        if video_id in processed['loom']:
            continue

        print(f"  å‡¦ç†ä¸­: {video_id}")
        result = get_loom_info(video_id)

        if result:
            output_file = TRANSCRIPTS_DIR / f"loom_{video_id}.json"

            data = {
                'type': 'loom',
                'video_id': video_id,
                'title': result.get('title', 'N/A'),
                'url': f"https://www.loom.com/share/{video_id}",
                'source_files': source_files,
                'processed_at': datetime.now().isoformat(),
                'transcript': None,  # Loom APIã§å–å¾—å¯èƒ½ãªå ´åˆã«è¿½åŠ 
                'note': 'Loomå‹•ç”»ã®æ–‡å­—èµ·ã“ã—ã«ã¯Loom Developer APIãŒå¿…è¦ã§ã™'
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            print(f"    â†’ ä¿å­˜: {output_file.name}")
            processed['loom'].append(video_id)
            new_count += 1

        time.sleep(0.5)

    save_processed(processed)
    print(f"\næ–°è¦å‡¦ç†: {new_count}ä»¶")

# ============ YouTubeå‡¦ç† ============

def get_youtube_transcript(video_id):
    """YouTubeå‹•ç”»ã®æ–‡å­—èµ·ã“ã—ã‚’å–å¾—"""
    try:
        from youtube_transcript_api import YouTubeTranscriptApi

        # æ—¥æœ¬èªâ†’è‹±èªâ†’è‡ªå‹•ã®é †ã§è©¦è¡Œ
        for lang in ['ja', 'en', None]:
            try:
                if lang:
                    transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])
                else:
                    transcript_list = YouTubeTranscriptApi.get_transcript(video_id)

                text = '\n'.join([t['text'] for t in transcript_list])
                return {
                    'transcript': text,
                    'language': lang or 'auto',
                }
            except:
                continue

    except ImportError:
        print("    youtube-transcript-apiæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    except Exception as e:
        pass

    return None

def process_youtube_videos():
    """å…¨YouTubeå‹•ç”»ã‚’å‡¦ç†"""
    print("\n" + "="*60)
    print("YouTubeå‹•ç”»ã®æ–‡å­—èµ·ã“ã—å‡¦ç†")
    print("="*60)

    processed = load_processed()

    # YouTubeå‹•ç”»IDã‚’æŠ½å‡º
    patterns = [
        re.compile(r'youtube\.com/watch\?v=([a-zA-Z0-9_-]+)'),
        re.compile(r'youtu\.be/([a-zA-Z0-9_-]+)'),
        re.compile(r'youtube\.com/shorts/([a-zA-Z0-9_-]+)'),
    ]

    youtube_videos = {}
    for md_file in NOTION_DOCS.rglob('*.md'):
        content = md_file.read_text(encoding='utf-8')
        for pattern in patterns:
            for match in pattern.finditer(content):
                video_id = match.group(1)
                if video_id not in youtube_videos:
                    youtube_videos[video_id] = []
                rel_path = str(md_file.relative_to(NOTION_DOCS))
                if rel_path not in youtube_videos[video_id]:
                    youtube_videos[video_id].append(rel_path)

    print(f"\nç™ºè¦‹ã—ãŸYouTubeå‹•ç”»: {len(youtube_videos)}ä»¶")

    new_count = 0
    skip_count = 0

    for video_id, source_files in youtube_videos.items():
        if video_id in processed['youtube']:
            continue

        print(f"  å‡¦ç†ä¸­: {video_id}")
        result = get_youtube_transcript(video_id)

        output_file = TRANSCRIPTS_DIR / f"youtube_{video_id}.json"

        data = {
            'type': 'youtube',
            'video_id': video_id,
            'url': f"https://www.youtube.com/watch?v={video_id}",
            'source_files': source_files,
            'processed_at': datetime.now().isoformat(),
        }

        if result and result.get('transcript'):
            data['transcript'] = result['transcript']
            data['language'] = result.get('language')
            print(f"    â†’ å­—å¹•å–å¾—æˆåŠŸ")
            new_count += 1
        else:
            data['transcript'] = None
            data['note'] = 'å­—å¹•ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'
            print(f"    â†’ å­—å¹•ãªã—")
            skip_count += 1

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        processed['youtube'].append(video_id)
        time.sleep(0.3)

    save_processed(processed)
    print(f"\næ–‡å­—èµ·ã“ã—æˆåŠŸ: {new_count}ä»¶ / å­—å¹•ãªã—: {skip_count}ä»¶")

# ============ PDFå‡¦ç† ============

def process_pdfs():
    """å…¨PDFã®æƒ…å ±ã‚’åé›†ï¼ˆS3ãƒªãƒ³ã‚¯ã¯æœŸé™åˆ‡ã‚Œã®ãŸã‚å–å¾—ä¸å¯ï¼‰"""
    print("\n" + "="*60)
    print("PDFæƒ…å ±åé›†")
    print("="*60)

    processed = load_processed()

    # S3 PDFãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
    pdf_pattern = re.compile(r'(https://prod-files-secure\.s3[^)\]\s]+\.pdf[^)\]\s]*)')

    pdfs = {}
    for md_file in NOTION_DOCS.rglob('*.md'):
        content = md_file.read_text(encoding='utf-8')
        for match in pdf_pattern.finditer(content):
            pdf_url = match.group(1)
            url_hash = get_file_hash(pdf_url)
            if url_hash not in pdfs:
                pdfs[url_hash] = {'url': pdf_url, 'files': []}
            rel_path = str(md_file.relative_to(NOTION_DOCS))
            if rel_path not in pdfs[url_hash]['files']:
                pdfs[url_hash]['files'].append(rel_path)

    print(f"\nç™ºè¦‹ã—ãŸPDF: {len(pdfs)}ä»¶")
    print("\nâš ï¸ Notionã®S3ãƒªãƒ³ã‚¯ã¯æœŸé™ä»˜ãã®ãŸã‚ã€PDFã®ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ä¸å¯")
    print("   â†’ Notion APIã‹ã‚‰ã®å†å–å¾—ãŒå¿…è¦ã§ã™")

    # PDFæƒ…å ±ã‚’JSONå½¢å¼ã§ä¿å­˜
    pdf_index_file = PDF_TEXTS_DIR / 'pdf_index.json'
    pdf_list = []

    for url_hash, data in pdfs.items():
        pdf_list.append({
            'hash': url_hash,
            'source_files': data['files'],
            'note': 'S3ãƒªãƒ³ã‚¯æœŸé™åˆ‡ã‚Œ - Notion APIã‹ã‚‰å†å–å¾—ãŒå¿…è¦'
        })

    with open(pdf_index_file, 'w', encoding='utf-8') as f:
        json.dump({'total': len(pdfs), 'pdfs': pdf_list}, f, indent=2, ensure_ascii=False)

    print(f"  â†’ PDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜: {pdf_index_file.name}")

# ============ ãƒªãƒ³ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç§»å‹• ============

def organize_link_contents():
    """æ—¢å­˜ã®link_contentãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†"""
    print("\n" + "="*60)
    print("ãƒªãƒ³ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ•´ç†")
    print("="*60)

    # notion_docså†…ã®link_contentãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆåŒ–
    link_files = list(NOTION_DOCS.rglob('link_*_content.txt'))
    print(f"\nç™ºè¦‹ã—ãŸãƒªãƒ³ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(link_files)}ä»¶")

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    link_index = []
    for link_file in link_files:
        rel_path = str(link_file.relative_to(NOTION_DOCS))
        link_index.append({
            'file': rel_path,
            'parent': str(link_file.parent.relative_to(NOTION_DOCS))
        })

    index_file = LINK_CONTENTS_DIR / 'link_index.json'
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump({'total': len(link_files), 'links': link_index}, f, indent=2, ensure_ascii=False)

    print(f"  â†’ ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜: {index_file.name}")
    print(f"\nâ€» ãƒªãƒ³ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ notion_docs/ ã«æ®‹ã—ã¾ã™ï¼ˆNotionã¨åŒæœŸã®ãŸã‚ï¼‰")

# ============ ãƒ¡ã‚¤ãƒ³ ============

def print_summary():
    """å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "="*60)
    print("å‡¦ç†ã‚µãƒãƒªãƒ¼")
    print("="*60)

    # knowledge_baseå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    loom_count = len(list(TRANSCRIPTS_DIR.glob('loom_*.json')))
    youtube_count = len(list(TRANSCRIPTS_DIR.glob('youtube_*.json')))

    # æ–‡å­—èµ·ã“ã—ãŒã‚ã‚‹YouTubeã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    youtube_with_transcript = 0
    for f in TRANSCRIPTS_DIR.glob('youtube_*.json'):
        with open(f, 'r') as jf:
            data = json.load(jf)
            if data.get('transcript'):
                youtube_with_transcript += 1

    print(f"""
knowledge_base/ å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«:

ğŸ“‚ transcripts/
  - Loomå‹•ç”»æƒ…å ±: {loom_count}ä»¶
  - YouTubeæƒ…å ±: {youtube_count}ä»¶ï¼ˆã†ã¡å­—å¹•ã‚ã‚Š: {youtube_with_transcript}ä»¶ï¼‰

ğŸ“‚ pdf_texts/
  - PDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: ã‚ã‚Š

ğŸ“‚ link_contents/
  - ãƒªãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: ã‚ã‚Š
""")

def main():
    print("="*60)
    print("Notion Docs ãƒ¡ãƒ‡ã‚£ã‚¢å‡¦ç†ãƒ„ãƒ¼ãƒ«")
    print("="*60)
    print(f"\nå…¥åŠ›: {NOTION_DOCS}")
    print(f"å‡ºåŠ›: {KNOWLEDGE_BASE}")

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    ensure_dirs()

    # å„å‡¦ç†ã‚’å®Ÿè¡Œ
    process_loom_videos()
    process_youtube_videos()
    process_pdfs()
    organize_link_contents()

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print_summary()

    print("\nâœ… å‡¦ç†å®Œäº†")
    print(f"\nå‡ºåŠ›å…ˆ: {KNOWLEDGE_BASE}")

if __name__ == '__main__':
    main()
